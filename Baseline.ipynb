{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Curation.CurationMain import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import traceback\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "#메캅\n",
    "import mecab_util\n",
    "import mecab\n",
    "mcb = mecab.MeCab()\n",
    "#정규식\n",
    "import re\n",
    "#슬랙\n",
    "from SendSlack import SlackClient\n",
    "global sc\n",
    "import Parallel\n",
    "sc = SlackClient()\n",
    "#패텍\n",
    "import sys\n",
    "import scipy\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoname = 'Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#보조함수들\n",
    "\n",
    "#doc-frq를 구해서 dic_df에 저장하는 알고리즘\n",
    "#포스태그된 list와 doc-frq를 저장할 dict를 입력받는다.\n",
    "#그리고 해당 행의 tf를 함께 구한다.\n",
    "def calculate_dicdf(poses, dic_df):\n",
    "    dic_tf = {}\n",
    "    for p in poses:\n",
    "        if p in dic_df:\n",
    "            dic_df[p]['df'] += 1            \n",
    "        else:\n",
    "            dic_df[p] = {'key':p, 'df':1}\n",
    "        if p in dic_tf:\n",
    "            dic_tf[p] += 1\n",
    "        else:\n",
    "            dic_tf[p] = 1\n",
    "    return dic_tf\n",
    "\n",
    "#dic_df와 tf를 입력받아 tfidf 값을 리턴한다.\n",
    "def calc_tfidf(dic_df, tf):\n",
    "    tfidf = {}    \n",
    "    for k, v in tf.items():        \n",
    "        tfidf[k] = {'key':k, 'tf':v, 'tfidf':v/dic_df[k]['idf'], 'idf':dic_df[k]['idf']}\n",
    "    return tfidf\n",
    "\n",
    "#docvec을 fasttext모델과 postag ordered로 구한다.\n",
    "def calcdocvec(model, ls_ordered):\n",
    "    ls = ls_ordered[:10]    \n",
    "    \n",
    "    #날아간 영어기사에 대한 임이의 값\n",
    "    if len(ls) == 0:\n",
    "        return model['this is an english random word pos']\n",
    "    \n",
    "    max_tfidf_val = max( [l['tfidf'] for l in ls] )\n",
    "    \n",
    "    vec = model[ls[0]['key']]\n",
    "    for tfidf in ls[:1]:\n",
    "        vec = vec + (model[tfidf['key']] * (tfidf['tfidf']/max_tfidf_val) )\n",
    "    return vec / len(ls)\n",
    "    #return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text를 입력받으면, 그녀석의 docvec을 구해준다.\n",
    "#필요한것 : fasttext model, doc-frq-dict, 최대 df, preprocess를 위한 copyright제거기, 불필요한 postag제거기\n",
    "def make_myvec(text, model, dic_df, maxdf, remove_copyright, stop_poses):\n",
    "    dicpos = {}\n",
    "    \n",
    "    #저작권 제거\n",
    "    tt = remove_copyright.sub('',text)\n",
    "    \n",
    "    #포스태깅\n",
    "    lsp = mecab_util.PosWithSpace(tt)\n",
    "    \n",
    "    #stop_pos 제거\n",
    "    lsp = [p for p in lsp if False not in [None == r.match(p[1]) for r in stop_poses]]\n",
    "    \n",
    "    #포스태그 합침\n",
    "    lsp = [p[0]+'/'+p[1] for p in lsp]\n",
    "    \n",
    "    #트리밍\n",
    "    lsp = [p for p in lsp if p != ' /SPACE']\n",
    "        \n",
    "    for p in lsp:        \n",
    "        if p in dicpos:\n",
    "            dicpos[p]['tf'] += 1\n",
    "        else:\n",
    "            dicpos[p] = {'key':p, 'tf':1}\n",
    "    \n",
    "    for k, v in dicpos.items():\n",
    "        dd = dic_df.get(p, None)\n",
    "\n",
    "        #자기의 df\n",
    "        mydf = dd['df'] if dd != None else 1\n",
    "        idf = mydf/maxdf\n",
    "        v['idf'] = idf\n",
    "        v['tfidf'] = v['tf']/v['idf']\n",
    "        \n",
    "    lspos = sorted([v for k, v in dicpos.items()], key=lambda x : x['tfidf'], reverse=True)    \n",
    "    #print(lspos[:5])\n",
    "    #print([l['tfidf'] for l in lspos[:5]] )\n",
    "    max_tfidf_val = max( [l['tfidf'] for l in lspos] )\n",
    "    #print([p['key'] for p in lspos[:10]])\n",
    "    vecs = [(p, model[p['key']]) for p in lspos[:10]]    \n",
    "    vec = vecs[0][1] * (vecs[0][0]['tfidf']/ max_tfidf_val)\n",
    "    for v in vecs[1:]:\n",
    "        vec = vec + v[1] * (v[0]['tfidf'] / max_tfidf_val)        \n",
    "    return vec/len(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar_text를 구해서 올려준다.\n",
    "#0 구할 문장\n",
    "#1 'docvec' 컬룸을 가진 글 dataframe\n",
    "#2 최대 몇개\n",
    "def similar(text, df_feed, topn=10):\n",
    "    vec = make_myvec(text, model, dic_df, maxdf, remove_copyright, stop_poses)\n",
    "    \n",
    "    df_feed['cos_sim'] = df_feed['docvec'].apply(lambda x : scipy.spatial.distance.cosine(x, vec))\n",
    "    dfdf = df_feed.sort_values('cos_sim', ascending=False)\n",
    "    return dfdf[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일단 저작권을 어떻게든 떼보려는 시도\n",
    "remove_copyright = re.compile('^\\s*[▲]*\\s*[\\(\\[](.*?)[\\)\\]]\\s*')\n",
    "\n",
    "#자를놈들, 명사 아닌놈, NNB, NR, NP\n",
    "stop_poses = [re.compile('^[^N]'), re.compile('[^A-Z]*NNB[^A-Z]*'), re.compile('[^A-Z]*NR[^A-Z]*'),re.compile('[^A-Z]*NP[^A-Z]*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#만들어진 dataframe 을 serializable하게 만든다.\n",
    "def make_df_serializable(df_in):\n",
    "    df = df_in[:]\n",
    "    for c in df.columns:\n",
    "        ls = list(df[c].apply(type).unique())\n",
    "        if list in ls or dict in ls or set in ls:\n",
    "            df[c] = df[c].apply(lambda x : json.dumps(x, ensure_ascii=False, default=str) if x != None else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제 learning함수\n",
    "def Learning():\n",
    "    #피드 리퀘스트 아티클을 다운받은다\n",
    "    df_origin = DocsFeedRequesters()\n",
    "    df = df_origin[:]\n",
    "    \n",
    "    #피드 등장할 아티클을 다운받는다\n",
    "    df_feed_origin = DocsFeedContents()\n",
    "    df_feed = df_feed_origin[:]\n",
    "    \n",
    "    df = df[df['contents'].notna()]\n",
    "    df_feed = df_feed[df_feed['contents'].notna()]\n",
    "    \n",
    "    #html tags와 저작권 표기 제거함\n",
    "    df['preprocessed_contents'] = df['contents'].apply(lambda x : remove_copyright.sub('', BeautifulSoup(x,'html5lib').text))\n",
    "\n",
    "    df_feed['preprocessed_contents'] = df_feed['contents'].apply(lambda x : remove_copyright.sub('', BeautifulSoup(x,'html5lib').text))\n",
    "    \n",
    "    #mecab_util 돌리고 단어/형태소명 으로 바꿈\n",
    "    df['pos'] = df['preprocessed_contents'].apply(lambda x : [f\"{t[0]}/{t[1]}\" for t in mecab_util.PosWithSpace(x, mcb)])\n",
    "    df_feed['pos'] = df_feed['preprocessed_contents'].apply(lambda x : [f\"{t[0]}/{t[1]}\" for t in mecab_util.PosWithSpace(x, mcb)])\n",
    "    \n",
    "    #스페이스바 제거\n",
    "    df['pos_trim'] = df['pos'].apply(lambda x : [t for t in x if t != ' /SPACE'])\n",
    "    df_feed['pos_trim'] = df_feed['pos'].apply(lambda x : [t for t in x if t != ' /SPACE'])\n",
    "    \n",
    "    #코퍼스 생성\n",
    "    corpus_list = list(df['pos']) + list(df_feed['pos'])\n",
    "    \n",
    "    #w2v 제작\n",
    "    import NLDate\n",
    "    vectorsz = 20\n",
    "    window = 10\n",
    "    model = FastText(size=vectorsz, window=5, min_count=1)\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin build vocab\")\n",
    "    model.build_vocab(sentences=corpus_list)\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin train\")\n",
    "    model.train(sentences=corpus_list, total_examples=len(corpus_list), epochs=10)\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "    \n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin preprocess\")\n",
    "    #자를 태그 찾아서 잘라냄\n",
    "    df['filtered_pos'] = df['pos_trim'].apply(lambda x : [t for t in x if False not in [None == r.match(t.split('/')[1]) for r in stop_poses]])\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "\n",
    "    #doc-frq를 구해서 저장한다.\n",
    "    dic_df = {}\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin doc-frq\")\n",
    "    df['tf'] = df['filtered_pos'].apply(lambda x : calculate_dicdf(x, dic_df))\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "    \n",
    "    #idf 계산\n",
    "    maxdf = len(df)\n",
    "    for k, v in dic_df.items():\n",
    "        v['idf'] = v['df']/maxdf\n",
    "\n",
    "    #1개의 dic_df 내용\n",
    "    print(dic_df[list(dic_df.keys())[0]])\n",
    "    \n",
    "    #tfidf를 계산한다.\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin tfidf\")\n",
    "    df['tfidf'] = df['tf'].apply(lambda x : calc_tfidf(dic_df, x))\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "    \n",
    "    #각 기사마다 tfidf 순으로 포스태깅단어를 소팅해서 알려준다.\n",
    "    df['tfidf_sorted'] = df['tfidf'].apply(lambda x : sorted([v for k, v in x.items()], key=lambda tv : tv['tfidf'], reverse=True))\n",
    "    \n",
    "    print(df['tfidf_sorted'].iloc[0][0])\n",
    "    \n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin docvec-all\")\n",
    "    df['docvec'] = df['tfidf_sorted'].apply(lambda x : calcdocvec(model, x) )\n",
    "\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "    \n",
    "    #feed의 docvec도 구한다.\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"begin docvec-feed\")\n",
    "    df_feed['docvec'] = df_feed['contents'].apply(lambda x : make_myvec( BeautifulSoup(x, 'html5lib').text, model, dic_df, maxdf, remove_copyright, stop_poses))\n",
    "    print(NLDate.Date(''))\n",
    "    print(\"fin\")\n",
    "    \n",
    "    df = df[df['docvec'].notna()]\n",
    "    df_feed = df_feed[df_feed['docvec'].notna()]\n",
    "    \n",
    "    #모델 저장\n",
    "    #0. 기존 모델 클리어\n",
    "    ClearAlgorithmCache(algoname)\n",
    "    \n",
    "    #1. 만든 fasttext w2v 모델을 디스크에 저장하고 upload한다.\n",
    "    print(\"save fasttext-model\")\n",
    "    model.save('/tmp/fasttext.model')\n",
    "    SaveAlgorithmCache(algoname, 'cache/fasttext.model', '/tmp/fasttext.model', filetype='file')\n",
    "    \n",
    "    #2. dic_df 를 저장한다\n",
    "    print(\"save dic_df\")\n",
    "    SaveAlgorithmCache(algoname, 'cache/dic_df.json', json.dumps(dic_df, ensure_ascii=False))\n",
    "    \n",
    "    #3. 데이터프레임 만들어놓은것들 저장한다\n",
    "    print(\"save dataframes to parquet\")    \n",
    "    \n",
    "    #여기서 pickling 해야함!!!!\n",
    "    try:\n",
    "        df['docvec'] = df['docvec'].apply(lambda x : json.dumps([float(t) for t in list(x)]) )\n",
    "        df_feed['docvec'] = df_feed['docvec'].apply(lambda x : json.dumps([float(t) for t in list(x)]))\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    \n",
    "    df = make_df_serializable(df)\n",
    "    df_feed = make_df_serializable(df_feed)\n",
    "    \n",
    "    try:\n",
    "        SaveAlgorithmCache(algoname, 'cache/df_feed.parquet', df_feed, filetype='dataframe')\n",
    "        SaveAlgorithmCache(algoname, 'cache/df.parquet', df, filetype='dataframe')\n",
    "    except:\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    print(\"save fin\")\n",
    "    \n",
    "    return model, df, df_feed, dic_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feed.to_parquet('df_feedtest.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('dftest.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 17:56:22.119490+09:00\n",
      "begin build vocab\n",
      "2020-03-08 17:57:56.032709+09:00\n",
      "begin train\n",
      "2020-03-08 18:36:21.447929+09:00\n",
      "fin\n",
      "2020-03-08 18:36:21.448223+09:00\n",
      "begin preprocess\n",
      "2020-03-08 18:39:06.044373+09:00\n",
      "fin\n",
      "2020-03-08 18:39:06.044615+09:00\n",
      "begin doc-frq\n",
      "2020-03-08 18:39:16.442438+09:00\n",
      "fin\n",
      "{'key': '배우/NNG', 'df': 6790, 'idf': 0.04953673305610272}\n",
      "2020-03-08 18:39:16.490034+09:00\n",
      "begin tfidf\n",
      "2020-03-08 18:39:27.913385+09:00\n",
      "fin\n",
      "{'key': '차문현/NNP', 'tf': 1, 'tfidf': 137070.0, 'idf': 7.295542423579193e-06}\n",
      "2020-03-08 18:39:38.216526+09:00\n",
      "begin docvec-all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 18:39:41.197506+09:00\n",
      "fin\n",
      "2020-03-08 18:39:41.197806+09:00\n",
      "begin docvec-feed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-08 18:44:59.026440+09:00\n",
      "fin\n",
      "save fasttext-model\n",
      "save dic_df\n",
      "save dataframes to parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-61:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tf/notebooks/libs/AWS/S3.py\", line 335, in Upload_Dataframe\n",
      "    s3.Object(bucketname, s3path).put(Body=data_bytes, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/boto3/resources/factory.py\", line 520, in do_action\n",
      "    response = action(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/boto3/resources/action.py\", line 83, in __call__\n",
      "    response = getattr(parent.meta.client, operation_name)(**params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/botocore/client.py\", line 276, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/botocore/client.py\", line 586, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save fin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-62:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tf/notebooks/libs/AWS/S3.py\", line 335, in Upload_Dataframe\n",
      "    s3.Object(bucketname, s3path).put(Body=data_bytes, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/boto3/resources/factory.py\", line 520, in do_action\n",
      "    response = action(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/boto3/resources/action.py\", line 83, in __call__\n",
      "    response = getattr(parent.meta.client, operation_name)(**params)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/botocore/client.py\", line 276, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/botocore/client.py\", line 586, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, df, df_feed, dic_df = None, None, None, None\n",
    "if __name__ == \"__main__\":\n",
    "    model, df, df_feed, dic_df = Learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodel = None\n",
    "dic_df = None\n",
    "\n",
    "def Init():\n",
    "    path = LoadAlgorithmCache(algoname, 'w2v_fasttext.model', filetype='file')\n",
    "    ftmodel = FastText.load(path)\n",
    "    dic_df = json.loads(LoadAlgorithmCache(algoname, 'dic_df.json'))\n",
    "    df = pd.DataFrame(LoadAlgorithmCache(algoname, 'dic_df.json'))\n",
    "\n",
    "\n",
    "#def Curation(**kwargs):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
